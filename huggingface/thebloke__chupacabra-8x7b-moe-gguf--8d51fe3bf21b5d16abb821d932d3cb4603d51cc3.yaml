- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q2_K.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q2_K.gguf
    sha256: 6c87558aa31d9e13c98a63975e453bfd6539d8addd8286448326b1b53186470b
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q2_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q2_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q3_K_M.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q3_K_M.gguf
    sha256: 78f24b6ab89ea5d96e0ff1494634fe43d33c1ae6a9029cadd79659eb01d808e2
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q3_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q3_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q4_0.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q4_0.gguf
    sha256: 0ffb5871861928d028c7f4bd0151c7a624cef3e2078bac668b87307f5c01a856
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q4_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q4_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q4_K_M.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q4_K_M.gguf
    sha256: 2746b25b9dd91856e789e6b365e47211591c5e53a7357b4b347e4b4a2285e310
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q4_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q4_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q5_0.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q5_0.gguf
    sha256: fca4ad69e6b2e142717fa83bfcda5ebf06336e19149da5bb78fa77739bd4e457
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q5_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q5_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q5_K_M.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q5_K_M.gguf
    sha256: dffa0ca92d7444ff0612fba75cd059ac7d3e1357364ba69d0c022939cd3a347a
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q5_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q5_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q6_K.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q6_K.gguf
    sha256: af002c8566c44036de0cc14a5f0d24d01bcee54f0db3b4f2a361bbfa4bfdbde1
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q6_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q6_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: chupacabra-8x7b-moe.Q8_0.gguf
    template:
      chat: thebloke__chupacabra-8x7b-moe-gguf
      completion: thebloke__chupacabra-8x7b-moe-gguf
  description: TheBloke/Chupacabra-8x7B-MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__chupacabra-8x7b-moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__chupacabra-8x7b-moe-gguf.tmpl
  - filename: chupacabra-8x7b-moe.Q8_0.gguf
    sha256: 73fd60551bd66044e86260dc990416bcf2c02595b21fa37b550f7146c644529f
    uri: 
      https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF/resolve/main/chupacabra-8x7b-moe.Q8_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: apache-2.0
  name: thebloke__chupacabra-8x7b-moe-gguf__chupacabra-8x7b-moe.Q8_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:perlthoughts/Chupacabra-8x7B-MoE
  - license:apache-2.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Chupacabra-8x7B-MoE-GGUF
