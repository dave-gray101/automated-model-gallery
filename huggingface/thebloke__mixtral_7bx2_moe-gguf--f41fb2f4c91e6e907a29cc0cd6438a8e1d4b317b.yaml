- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q2_K.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q2_K.gguf
    sha256: 53549dc6b76a4c9dd8e02a40ee19bb723eae2ea8bde57ffc37ef671640d386d8
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q2_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q2_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q3_K_M.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q3_K_M.gguf
    sha256: 169af88ec1a246ad1e9ad60b5605e4e269f56e38718baaa91e81804216d5b4f1
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q3_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q3_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q4_0.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q4_0.gguf
    sha256: 7143d22f633cfb2d89071c604f0b2f99361c715a986f8e7e4f50098d6742b786
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q4_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q4_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q4_K_M.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q4_K_M.gguf
    sha256: c95aa92dc9f1a9ad92e0b43ccf47be80dd1a3587fefc42e281477fb9e5d613c5
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q4_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q4_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q5_0.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q5_0.gguf
    sha256: 0733ed867863e6a72f242b9ff2d7a6444381cfe273b72f3c6a39543ee323506f
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q5_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q5_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q5_K_M.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q5_K_M.gguf
    sha256: 1a0ea017f63b19bf7956ffd94c402df774192abd19583204a5310d616a7025f6
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q5_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q5_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q6_K.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q6_K.gguf
    sha256: 91f995ee1939e3517579bb6ef52c5261dcfa4d9b31b3693e35114413e150ad6e
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q6_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q6_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_7bx2_moe.Q8_0.gguf
    template:
      chat: thebloke__mixtral_7bx2_moe-gguf
      completion: thebloke__mixtral_7bx2_moe-gguf
  description: TheBloke/Mixtral_7Bx2_MoE-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_7bx2_moe-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_7bx2_moe-gguf.tmpl
  - filename: mixtral_7bx2_moe.Q8_0.gguf
    sha256: 380ea6ff565cfa990f68f295c6e9bcb70ddfb928e042a22b45a4495f6432f518
    uri: 
      https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/resolve/main/mixtral_7bx2_moe.Q8_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_7bx2_moe-gguf__mixtral_7bx2_moe.Q8_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_7Bx2_MoE
  - license:cc-by-nc-4.0
  - has_space
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF
