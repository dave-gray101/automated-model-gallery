- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q2_K.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q2_K.gguf
    sha256: ffbd899f6846040c06beb85bd850df2a48ebf294390a7bdf70ee61bc7e485044
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q2_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q2_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q3_K_M.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q3_K_M.gguf
    sha256: ce68ac615aa5b1fa610ad76e21443009d3e12a60988f4163186c02c63d449b76
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q3_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q3_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q4_0.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q4_0.gguf
    sha256: dc680bef5d281702ab37d9802e179b8a5cb03a9dcd56ed9a4f297b4865c29fb1
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q4_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q4_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q4_K_M.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q4_K_M.gguf
    sha256: 1d7e14e10fc916b43b902e08fcfb02549bbff5639d1421069cd63634dbd59dac
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q4_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q4_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q5_0.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q5_0.gguf
    sha256: 77720cf3d813dfb0288324f59bb79265215ed1a68d59870a9f93b7cfe18058a9
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q5_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q5_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q5_K_M.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q5_K_M.gguf
    sha256: 537855aa095f636692fe7970f44b1d855b77f428a11c3cb542a86bacb36949e8
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q5_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q5_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q6_K.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q6_K.gguf
    sha256: 316930041393658b0cafad4523fc3a44a21412ec3b9fd6499b64ca4105683e15
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q6_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q6_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: 8x7b-moe-test-not-mixtral.Q8_0.gguf
    template:
      chat: chatml
      completion: chatml
  description: TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: chatml.tmpl
    sha256: 96edc939f785d5cf5b35abb7289f60e0ad92b3f6fbd75f4f95647f559af5fa2e
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/chatml.tmpl
  - filename: 8x7b-moe-test-not-mixtral.Q8_0.gguf
    sha256: b87f6bc1bfebc8a685373c61b246018c4803dbbc99739cb040a1f3258680b8e9
    uri: 
      https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF/resolve/main/8x7b-moe-test-not-mixtral.Q8_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: gpl-3.0
  name: thebloke__8x7b-moe-test-not-mixtral-gguf__8x7b-moe-test-not-mixtral.Q8_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - qwen
  - en
  - zh
  - base_model:CausalLM/8x7B-MoE-test-NOT-MIXTRAL
  - license:gpl-3.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/8x7B-MoE-test-NOT-MIXTRAL-GGUF
