- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q2_K.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q2_K.gguf
    sha256: 38bd1847529ce484ee3d7d4487b26fd3699262cd4a2edda78df2660046833514
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q2_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q2_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q3_K_M.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q3_K_M.gguf
    sha256: 2c435883766d35f10d4e7fd971b238051f688784cc14b302e4e3cd80f34bd267
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q3_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q3_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q4_0.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q4_0.gguf
    sha256: f9d61374f4f96414406aee3520b2bbb932899bbbfe2a450fd540741d0b940cd0
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q4_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q4_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q4_K_M.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q4_K_M.gguf
    sha256: 0ea4500fc0ad20867dcf5a281e13362dc4072c2a07c1a000d78c70f978dd08b9
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q4_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q4_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q5_0.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q5_0.gguf
    sha256: 0c4d5c49f69588dd85ad80010c0c9418c299f475474c75bd2217ebfc07ac0e56
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q5_0.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q5_0.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q5_K_M.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q5_K_M.gguf
    sha256: ac926afdc02b179f68e72ac69810485593aec9c442f63e6cb21893bb28a3887e
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q5_K_M.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q5_K_M.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: mixtral_34bx2_moe_60b.Q6_K.gguf
    template:
      chat: thebloke__mixtral_34bx2_moe_60b-gguf
      completion: thebloke__mixtral_34bx2_moe_60b-gguf
  description: TheBloke/Mixtral_34Bx2_MoE_60B-GGUF - llamaFileFormatFallback configuration
  files:
  - filename: thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
    sha256: e1a2961994016082e8bd585199ff1e1dc0e8ef2b1596da49b38cc99785321e22
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/thebloke__mixtral_34bx2_moe_60b-gguf.tmpl
  - filename: mixtral_34bx2_moe_60b.Q6_K.gguf
    sha256: 6aadd80a2069257f550c6268f268587f9fcf092b1fc6a480acc5bfba2f8dae02
    uri: 
      https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF/resolve/main/mixtral_34bx2_moe_60b.Q6_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: cc-by-nc-4.0
  name: thebloke__mixtral_34bx2_moe_60b-gguf__mixtral_34bx2_moe_60b.Q6_K.gguf
  tags:
  - transformers
  - gguf
  - mixtral
  - base_model:cloudyu/Mixtral_34Bx2_MoE_60B
  - license:cc-by-nc-4.0
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF
